# ディープラーニングの基礎

## 1. ニューラルネットワークの概念
- **パーセプトロン**: 入力の線形結合に活性化関数を適用して出力を得る最小単位。重みとバイアスを調整することで線形分離可能な問題を解く。
- **多層パーセプトロン (MLP)**: パーセプトロンを複数層に積み重ねることで非線形な関係を表現可能にしたモデル。入力層・中間層・出力層で構成される。
- **表現学習**: 隠れ層が入力データを抽象化し、タスクに有用な特徴量を自動的に獲得する。

## 2. 順伝播と逆伝播
- **順伝播 (Forward Propagation)**: 入力をネットワークに順に通し、各層で線形変換と活性化を適用して出力を得る処理。
- **損失関数**: 予測と正解の誤差を数値化する関数。分類ではクロスエントロピー、回帰では平均二乗誤差などが一般的。
- **逆伝播 (Backpropagation)**: 損失関数をパラメータで微分し、連鎖律で勾配を各層へ伝播するアルゴリズム。重み更新のための勾配を効率的に計算できる。

## 3. 活性化関数
- **ReLU (Rectified Linear Unit)**: `ReLU(x) = max(0, x)`。勾配消失を緩和し学習を高速化するが、負の入力で勾配が 0 になる「死んだ ReLU」問題がある。
- **シグモイド / tanh**: 値を 0〜1、-1〜1 に圧縮する。深いネットワークでは勾配消失が起きやすい。
- **Swish / GELU**: 近年のモデルで利用される滑らかな活性化関数。ReLU より表現力が高いとされるケースがある。

## 4. 最適化手法
- **確率的勾配降下法 (SGD)**: ミニバッチ単位で勾配を計算しながらパラメータを更新。シンプルで汎用性が高い。
- **モーメンタム付き SGD**: 勾配の移動平均を利用して収束を安定化・高速化する。
- **Adam**: 勾配の一次・二次モーメントを推定し、各パラメータごとに学習率を調整する。初期学習の収束が速い。

## 5. 過学習と正則化
- **正則化**: L1/L2 正則化や重み減衰でパラメータの大きさを抑制し、モデルの複雑さをコントロール。
- **ドロップアウト**: 学習時にランダムにユニットを無効化し、汎化性能を高める。
- **データ拡張**: 教師データを拡張して多様性を確保。画像なら回転・平行移動、テキストなら置換・反転など。
- **早期終了**: 検証データの損失が悪化し始めた時点で学習を止め、過学習を防ぐ。

## 6. 代表的なアーキテクチャ
- **畳み込みニューラルネットワーク (CNN)**: 畳み込みとプーリングで局所的な特徴を抽出し、画像認識や動画処理で高精度を実現。
- **再帰型ニューラルネットワーク (RNN)**: シーケンスデータの時間的依存関係を扱う。長期依存を学習しやすい LSTM や GRU が代表的。
- **Transformer**: 自己注意機構により長距離依存を効率的に処理。自然言語処理やマルチモーダルなタスクで主流となっている。

## 7. 学習の実践ポイント
- **データ前処理**: 正規化・標準化・欠損値処理などを適切に行う。入力スケールが整うと学習が安定する。
- **ハイパーパラメータ探索**: 学習率、バッチサイズ、層数、ユニット数などをグリッド探索やベイズ最適化で調整。
- **評価指標の選択**: タスクに適した指標 (精度、F1 スコア、ROC AUC、BLEU など) を用いてモデル性能を判断。
- **再現性確保**: 乱数シードの固定、バージョン管理、実験ログの記録が重要。

## 8. 今後の学習へのステップ
1. **フレームワーク習熟**: PyTorch や TensorFlow を使って基本的なモデルを実装する。
2. **論文リーディング**: 代表的な研究 (AlexNet, ResNet, BERT など) を読み、設計思想や改良点を理解する。
3. **応用プロジェクト**: 画像分類・自然言語処理・音声認識など興味のある領域で小規模なプロジェクトを構築し、モデル選択や評価の経験を積む。

---
ディープラーニングは幅広い領域に応用可能な強力な手法ですが、データ品質や評価の設計が結果を大きく左右します。基礎概念を押さえつつ、実装と検証を繰り返して理解を深めていくことが重要です。
